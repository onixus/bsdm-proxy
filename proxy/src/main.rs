use base64::engine::general_purpose;
use base64::Engine;  // –¢—Ä–µ–π—Ç –¥–ª—è decode
use bytes::Bytes;
use hyper::body::Incoming;
use hyper::header::{HeaderName, HeaderValue, AUTHORIZATION};
use hyper::server::conn::http1;
use hyper::service::service_fn;
use hyper::{Method, Request, Response, StatusCode};
use hyper_util::rt::TokioIo;
use quick_cache::sync::Cache;
use rcgen::{
    BasicConstraints, Certificate, CertificateParams, DistinguishedName, DnType, IsCa, KeyPair,
    KeyUsagePurpose,
};
use rdkafka::config::ClientConfig;
use rdkafka::producer::{FutureProducer, FutureRecord};
use serde::Serialize;
use sha2::{Digest, Sha256};
use std::collections::HashMap;
use std::net::SocketAddr;
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime};
use tokio::io::copy_bidirectional;
use tokio::net::{TcpListener, TcpStream};
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

type CertPair = (Bytes, Bytes);
type CertMap = Arc<RwLock<HashMap<Arc<str>, CertPair>>>;
type Body = http_body_util::Full<Bytes>;

const CACHEABLE_METHODS: &[&str] = &["GET", "HEAD"];
const CACHEABLE_STATUS_CODES: &[u16] = &[200, 203, 204, 206, 300, 301, 404, 405, 410, 414, 501];

/// –ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π HTTP –æ—Ç–≤–µ—Ç (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è)
#[derive(Clone, Debug)]
struct CachedResponse {
    status: u16,
    headers: Arc<[(Arc<str>, Arc<str>)]>,  // Arc –¥–ª—è zero-copy clone
    body: Bytes,  // Bytes —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Arc –≤–Ω—É—Ç—Ä–∏
    cached_at: SystemTime,
    ttl: Duration,
}

impl CachedResponse {
    #[inline]
    fn is_expired(&self) -> bool {
        SystemTime::now()
            .duration_since(self.cached_at)
            .map_or(true, |age| age > self.ttl)
    }

    fn to_response(&self) -> Response<Body> {
        let mut response = Response::new(Body::new(self.body.clone()));
        *response.status_mut() = StatusCode::from_u16(self.status).unwrap_or(StatusCode::OK);

        let headers_mut = response.headers_mut();
        for (key, value) in self.headers.iter() {
            if let (Ok(name), Ok(val)) = (
                HeaderName::from_bytes(key.as_bytes()),
                HeaderValue::from_str(value),
            ) {
                headers_mut.insert(name, val);
            }
        }

        headers_mut.insert("x-cache-status", HeaderValue::from_static("HIT"));
        response
    }
}

/// –ú–µ–Ω–µ–¥–∂–µ—Ä —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω —Å Arc<str> –∫–ª—é—á–∞–º–∏)
#[derive(Clone)]
struct CertCache {
    certs: CertMap,
    ca_cert: Arc<Certificate>,
    ca_key: Arc<KeyPair>,
}

impl CertCache {
    fn new(ca_key_pem: Vec<u8>) -> Self {
        let ca_key = Arc::new(
            KeyPair::from_pem(&String::from_utf8_lossy(&ca_key_pem))
                .expect("CA key parse failed"),
        );

        let mut ca_params = CertificateParams::new(vec!["BSDM Proxy CA".to_string()])
            .expect("Failed to create CA params");
        ca_params.is_ca = IsCa::Ca(BasicConstraints::Unconstrained);
        ca_params.key_usages = vec![
            KeyUsagePurpose::KeyCertSign,
            KeyUsagePurpose::DigitalSignature,
        ];
        ca_params.distinguished_name = DistinguishedName::new();
        ca_params
            .distinguished_name
            .push(DnType::CommonName, "BSDM Proxy CA");

        let ca_cert = Arc::new(
            ca_params
                .self_signed(&ca_key)
                .expect("CA cert instance failed"),
        );

        Self {
            certs: Arc::new(RwLock::new(HashMap::new())),
            ca_cert,
            ca_key,
        }
    }

    async fn get_or_generate(&self, domain: &str) -> Result<CertPair, Box<dyn std::error::Error>> {
        let domain_arc: Arc<str> = domain.into();
        
        // –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å read lock
        {
            let cache = self.certs.read().await;
            if let Some(cert) = cache.get(&domain_arc) {
                debug!("Certificate cache HIT for {}", domain);
                return Ok(cert.clone());
            }
        }

        debug!("Certificate cache MISS for {}, generating...", domain);
        let key_pair = KeyPair::generate()?;
        let mut params = CertificateParams::new(vec![domain.to_string()])?;
        params.distinguished_name = DistinguishedName::new();
        params.distinguished_name.push(DnType::CommonName, domain);
        params
            .distinguished_name
            .push(DnType::OrganizationName, "BSDM Proxy");

        let cert = params.self_signed(&key_pair)?;
        let cert_pem = Bytes::from(cert.pem().into_bytes());
        let key_pem = Bytes::from(key_pair.serialize_pem().into_bytes());

        let cert_pair = (cert_pem, key_pem);
        let mut cache = self.certs.write().await;
        cache.insert(domain_arc, cert_pair.clone());
        Ok(cert_pair)
    }
}

/// –°–æ–±—ã—Ç–∏–µ –¥–ª—è Kafka (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏)
#[derive(Serialize, Clone, Debug)]
struct CacheEvent {
    url: String,
    method: String,
    status: u16,
    cache_key: String,
    cache_status: &'static str,
    timestamp: u64,
    #[serde(skip_serializing_if = "HashMap::is_empty")]
    headers: HashMap<String, String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    user_id: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    username: Option<String>,
    client_ip: String,
    domain: String,
    response_size: u64,
    request_duration_ms: u64,
    #[serde(skip_serializing_if = "Option::is_none")]
    content_type: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    user_agent: Option<String>,
}

/// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–µ—à–∞
#[derive(Clone)]
struct CacheConfig {
    capacity: usize,
    default_ttl: Duration,
    max_body_size: usize,  // –ù–æ–≤–æ–µ: –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ body –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            capacity: 10_000,
            default_ttl: Duration::from_secs(3600),
            max_body_size: 10 * 1024 * 1024,  // 10MB
        }
    }
}

/// –ì–ª–∞–≤–Ω—ã–π –ø—Ä–æ–∫—Å–∏ —Å–µ—Ä–≤–∏—Å
#[derive(Clone)]
struct ProxyService {
    cert_cache: CertCache,
    http_cache: Arc<Cache<Arc<str>, CachedResponse>>,
    cache_config: CacheConfig,
    kafka_producer: Option<Arc<FutureProducer>>,
    http_client: hyper_util::client::legacy::Client<hyper_util::client::legacy::connect::HttpConnector, Body>,
}

impl ProxyService {
    fn new(
        cert_cache: CertCache,
        cache_config: CacheConfig,
        kafka_brokers: Option<String>,
    ) -> Self {
        let kafka_producer = kafka_brokers.and_then(|brokers| {
            ClientConfig::new()
                .set("bootstrap.servers", &brokers)
                .set("message.timeout.ms", "5000")
                .set("compression.type", "snappy")
                .set("batch.size", "32768")  // –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –ª—É—á—à–µ–≥–æ batching
                .set("linger.ms", "5")  // –£–º–µ–Ω—å—à–µ–Ω –¥–ª—è –º–µ–Ω—å—à–µ–π –∑–∞–¥–µ—Ä–∂–∫–∏
                .set("acks", "0")  // Fire-and-forget –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
                .create()
                .ok()
                .map(Arc::new)
        });

        let http_cache = Arc::new(Cache::new(cache_config.capacity));
        
        // –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π HTTP –∫–ª–∏–µ–Ω—Ç —Å connection pooling
        let http_client = hyper_util::client::legacy::Client::builder(hyper_util::rt::TokioExecutor::new())
            .pool_idle_timeout(Duration::from_secs(90))
            .pool_max_idle_per_host(32)
            .build_http();

        Self {
            cert_cache,
            http_cache,
            cache_config,
            kafka_producer,
            http_client,
        }
    }

    #[inline]
    fn generate_cache_key(&self, method: &str, url: &str) -> Arc<str> {
        let mut hasher = Sha256::new();
        hasher.update(method.as_bytes());
        hasher.update(b":");
        hasher.update(url.as_bytes());
        hex::encode(hasher.finalize()).into()
    }

    #[inline]
    fn is_cacheable(&self, method: &str, status: u16, body_size: usize) -> bool {
        CACHEABLE_METHODS.contains(&method)
            && CACHEABLE_STATUS_CODES.contains(&status)
            && body_size <= self.cache_config.max_body_size
    }

    #[inline]
    fn extract_domain(url_str: &str) -> String {
        url::Url::parse(url_str)
            .ok()
            .and_then(|u| u.host().map(|h| h.to_string()))
            .unwrap_or_else(|| "unknown".to_string())
    }

    fn extract_user_info(req: &Request<Incoming>) -> (Option<String>, Option<String>) {
        if let Some(auth_header) = req.headers().get(AUTHORIZATION) {
            if let Ok(auth_str) = auth_header.to_str() {
                if let Some(encoded) = auth_str.strip_prefix("Basic ") {
                    if let Ok(decoded_bytes) = general_purpose::STANDARD.decode(encoded) {
                        if let Ok(credentials) = String::from_utf8(decoded_bytes) {
                            if let Some((username, _)) = credentials.split_once(':') {
                                return (Some(username.to_string()), Some(username.to_string()));
                            }
                        }
                    }
                }
            }
        }
        (None, None)
    }

    // –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ Kafka –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    fn send_to_kafka_async(&self, event: CacheEvent) {
        if let Some(producer) = self.kafka_producer.clone() {
            tokio::spawn(async move {
                match serde_json::to_string(&event) {
                    Ok(payload) => {
                        let record = FutureRecord::to("cache-events")
                            .payload(&payload)
                            .key(&event.cache_key);
                        if let Err((e, _)) = producer.send(record, Duration::ZERO).await {
                            warn!("Kafka send failed: {}", e);
                        }
                    }
                    Err(e) => error!("Event serialization failed: {}", e),
                }
            });
        }
    }

    async fn handle_request(
        &self,
        req: Request<Incoming>,
        client_ip: String,
    ) -> Result<Response<Body>, Box<dyn std::error::Error + Send + Sync>> {
        let request_start = Instant::now();
        let method = req.method().to_string();
        let uri = req.uri().clone();
        let url = uri.to_string();
        let (user_id, username) = Self::extract_user_info(&req);
        let cache_key = self.generate_cache_key(&method, &url);

        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
        if let Some(cached) = self.http_cache.get(&cache_key) {
            if !cached.is_expired() {
                info!("Cache HIT: {} {}", method, url);
                
                let event = CacheEvent {
                    url: url.clone(),
                    method: method.clone(),
                    status: cached.status,
                    cache_key: cache_key.to_string(),
                    cache_status: "HIT",
                    timestamp: SystemTime::now().duration_since(SystemTime::UNIX_EPOCH)?.as_secs(),
                    headers: HashMap::new(),  // –ü—É—Å—Ç–æ–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    user_id: user_id.clone(),
                    username: username.clone(),
                    client_ip: client_ip.clone(),
                    domain: Self::extract_domain(&url),
                    response_size: cached.body.len() as u64,
                    request_duration_ms: request_start.elapsed().as_millis() as u64,
                    content_type: cached.headers.iter()
                        .find(|(k, _)| k.eq_ignore_ascii_case("content-type"))
                        .map(|(_, v)| v.to_string()),
                    user_agent: None,
                };
                
                self.send_to_kafka_async(event);
                return Ok(cached.to_response());
            }
        }

        info!("Cache MISS: {} {}", method, url);
        
        // –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Incoming –≤ Body –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞
        let (parts, body) = req.into_parts();
        let body_bytes = http_body_util::BodyExt::collect(body).await?.to_bytes();
        let req = Request::from_parts(parts, Body::new(body_bytes));
        
        // –ó–∞–ø—Ä–æ—Å –∫ upstream —Å –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º –∫–ª–∏–µ–Ω—Ç–æ–º
        match self.http_client.request(req).await {
            Ok(response) => {
                let status = response.status();
                let headers_map: HashMap<String, String> = response
                    .headers()
                    .iter()
                    .filter_map(|(k, v)| v.to_str().ok().map(|v| (k.as_str().to_string(), v.to_string())))
                    .collect();

                let body_bytes = http_body_util::BodyExt::collect(response.into_body())
                    .await?
                    .to_bytes();
                let body_size = body_bytes.len();
                
                let cache_status = if self.is_cacheable(&method, status.as_u16(), body_size) {
                    // –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: Arc –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    let headers_arc: Arc<[(Arc<str>, Arc<str>)]> = headers_map
                        .iter()
                        .map(|(k, v)| (Arc::from(k.as_str()), Arc::from(v.as_str())))
                        .collect();
                    
                    let cached_response = CachedResponse {
                        status: status.as_u16(),
                        headers: headers_arc,
                        body: body_bytes.clone(),
                        cached_at: SystemTime::now(),
                        ttl: self.cache_config.default_ttl,
                    };
                    self.http_cache.insert(cache_key.clone(), cached_response);
                    "MISS"
                } else {
                    "BYPASS"
                };

                let event = CacheEvent {
                    url: url.clone(),
                    method: method.clone(),
                    status: status.as_u16(),
                    cache_key: cache_key.to_string(),
                    cache_status,
                    timestamp: SystemTime::now().duration_since(SystemTime::UNIX_EPOCH)?.as_secs(),
                    headers: headers_map.clone(),
                    user_id,
                    username,
                    client_ip,
                    domain: Self::extract_domain(&url),
                    response_size: body_size as u64,
                    request_duration_ms: request_start.elapsed().as_millis() as u64,
                    content_type: headers_map.get("content-type").cloned(),
                    user_agent: headers_map.get("user-agent").cloned(),
                };
                
                self.send_to_kafka_async(event);

                let mut resp = Response::new(Body::new(body_bytes));
                *resp.status_mut() = status;
                for (key, value) in headers_map {
                    if let (Ok(name), Ok(val)) = (
                        HeaderName::from_bytes(key.as_bytes()),
                        HeaderValue::from_str(&value),
                    ) {
                        resp.headers_mut().insert(name, val);
                    }
                }
                Ok(resp)
            }
            Err(e) => {
                error!("Upstream error: {}", e);
                let mut response = Response::new(Body::new(Bytes::from_static(b"502 Bad Gateway")));
                *response.status_mut() = StatusCode::BAD_GATEWAY;
                Ok(response)
            }
        }
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt()
        .with_env_filter(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| "info,bsdm_proxy=debug".into()),
        )
        .init();

    let ca_key = tokio::fs::read("/certs/ca.key").await?;
    let cert_cache = CertCache::new(ca_key);
    let kafka_brokers = std::env::var("KAFKA_BROKERS").ok();
    let cache_capacity = std::env::var("CACHE_CAPACITY")
        .ok()
        .and_then(|s| s.parse().ok())
        .unwrap_or(10_000);
    let cache_ttl_secs = std::env::var("CACHE_TTL_SECONDS")
        .ok()
        .and_then(|s| s.parse().ok())
        .unwrap_or(3600);
    let max_body_size = std::env::var("MAX_CACHE_BODY_SIZE")
        .ok()
        .and_then(|s| s.parse().ok())
        .unwrap_or(10 * 1024 * 1024);  // 10MB default

    let cache_config = CacheConfig {
        capacity: cache_capacity,
        default_ttl: Duration::from_secs(cache_ttl_secs),
        max_body_size,
    };

    let service = Arc::new(ProxyService::new(cert_cache, cache_config.clone(), kafka_brokers));
    let http_port = std::env::var("HTTP_PORT")
        .ok()
        .and_then(|s| s.parse().ok())
        .unwrap_or(1488);

    let listener = TcpListener::bind(format!("0.0.0.0:{}", http_port)).await?;
    info!("üöÄ BSDM-Proxy v2.0 (optimized) on 0.0.0.0:{}", http_port);
    info!("üì¶ Cache: {} entries, TTL: {:?}, max body: {}MB", 
        service.http_cache.capacity(), 
        cache_config.default_ttl,
        max_body_size / 1024 / 1024
    );

    loop {
        let (stream, addr) = listener.accept().await?;
        let service_clone = service.clone();
        let client_ip = addr.ip().to_string();
        
        tokio::spawn(async move {
            handle_connection(stream, addr, service_clone, client_ip).await;
        });
    }
}

async fn handle_connection(
    stream: TcpStream,
    addr: SocketAddr,
    service: Arc<ProxyService>,
    client_ip: String,
) {
    let io = TokioIo::new(stream);
    let svc = service_fn(move |req: Request<Incoming>| {
        let service = service.clone();
        let client_ip = client_ip.clone();
        let request_start = Instant::now();
        
        async move {
            if req.method() == Method::CONNECT {
                let authority = req.uri().authority()
                    .ok_or("Missing authority")?
                    .as_str()
                    .to_string();
                
                tokio::spawn({
                    let service = service.clone();
                    let client_ip = client_ip.clone();
                    async move {
                        match hyper::upgrade::on(req).await {
                            Ok(upgraded) => {
                                // –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º Upgraded –≤ TokioIo –¥–ª—è AsyncRead/AsyncWrite
                                let mut client_io = TokioIo::new(upgraded);
                                
                                match TcpStream::connect(&authority).await {
                                    Ok(mut upstream) => {
                                        // Bidirectional copy –º–µ–∂–¥—É –∫–ª–∏–µ–Ω—Ç–æ–º –∏ upstream
                                        match copy_bidirectional(&mut client_io, &mut upstream).await {
                                            Ok((bytes_c2u, bytes_u2c)) => {
                                                let duration_ms = request_start.elapsed().as_millis() as u64;
                                                let domain = authority.split(':').next().unwrap_or("unknown").to_string();
                                                
                                                let event = CacheEvent {
                                                    url: format!("https://{}", authority),
                                                    method: "CONNECT".to_string(),
                                                    status: 200,
                                                    cache_key: service.generate_cache_key("CONNECT", &authority).to_string(),
                                                    cache_status: "BYPASS",
                                                    timestamp: SystemTime::now()
                                                        .duration_since(SystemTime::UNIX_EPOCH)
                                                        .unwrap_or_default()
                                                        .as_secs(),
                                                    headers: HashMap::new(),
                                                    user_id: None,
                                                    username: None,
                                                    client_ip,
                                                    domain,
                                                    response_size: bytes_u2c,
                                                    request_duration_ms: duration_ms,
                                                    content_type: None,
                                                    user_agent: None,
                                                };
                                                
                                                service.send_to_kafka_async(event);
                                                debug!("CONNECT closed: {}‚Üë {}‚Üì", bytes_c2u, bytes_u2c);
                                            }
                                            Err(e) => error!("CONNECT copy failed: {}", e),
                                        }
                                    }
                                    Err(e) => error!("CONNECT upstream failed: {}", e),
                                }
                            }
                            Err(e) => error!("Upgrade failed: {}", e),
                        }
                    }
                });
                
                let response = Response::builder()
                    .status(StatusCode::OK)
                    .body(Body::new(Bytes::new()))?;
                return Ok::<_, Box<dyn std::error::Error + Send + Sync>>(response);
            }
            service.handle_request(req, client_ip).await
        }
    });

    if let Err(e) = http1::Builder::new()
        .preserve_header_case(true)
        .title_case_headers(true)
        .serve_connection(io, svc)
        .with_upgrades()
        .await
    {
        error!("Connection error from {}: {}", addr, e);
    }
}
